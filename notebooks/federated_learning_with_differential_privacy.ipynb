{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "rtgStTrNIId-"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "\n",
        "import dp_accounting\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8xl6I2X9ObS"
      },
      "source": [
        "## Download and preprocess the federated EMNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "EdY72mGKJqi0"
      },
      "outputs": [],
      "source": [
        "def get_emnist_dataset():\n",
        "  emnist_train, emnist_test = tff.simulation.datasets.emnist.load_data(\n",
        "      only_digits=True)\n",
        "\n",
        "  def element_fn(element):\n",
        "    return collections.OrderedDict(\n",
        "        x=tf.expand_dims(element['pixels'], -1), y=element['label'])\n",
        "\n",
        "  def preprocess_train_dataset(dataset):\n",
        "    # Use buffer_size same as the maximum client dataset size,\n",
        "    # 418 for Federated EMNIST\n",
        "    return (dataset.map(element_fn)\n",
        "                   .shuffle(buffer_size=418)\n",
        "                   .repeat(1)\n",
        "                   .batch(32, drop_remainder=False))\n",
        "\n",
        "  def preprocess_test_dataset(dataset):\n",
        "    return dataset.map(element_fn).batch(128, drop_remainder=False)\n",
        "\n",
        "  emnist_train = emnist_train.preprocess(preprocess_train_dataset)\n",
        "  emnist_test = preprocess_test_dataset(\n",
        "      emnist_test.create_tf_dataset_from_all_clients())\n",
        "  return emnist_train, emnist_test\n",
        "\n",
        "train_data, test_data = get_emnist_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ntJ5coIJxS2"
      },
      "source": [
        "## Define our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "YK_UGq_0KGMX"
      },
      "outputs": [],
      "source": [
        "def my_model_fn():\n",
        "  model = tf.keras.models.Sequential([\n",
        "      tf.keras.layers.Reshape(input_shape=(28, 28, 1), target_shape=(28 * 28,)),\n",
        "      tf.keras.layers.Dense(200, activation=tf.nn.relu),\n",
        "      tf.keras.layers.Dense(200, activation=tf.nn.relu),\n",
        "      tf.keras.layers.Dense(10)])\n",
        "  return tff.learning.models.from_keras_model(\n",
        "      keras_model=model,\n",
        "      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "      input_spec=test_data.element_spec,\n",
        "      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-BZ_L4GMmXP"
      },
      "outputs": [],
      "source": [
        "def train(rounds, noise_multiplier, clients_per_round, data_frame):\n",
        "    # Increase clipped_count_stddev to a larger value for stability\n",
        "    clipped_count_stddev = max(1.0, 0.5 * clients_per_round)  # increase multiplier here\n",
        "    \n",
        "    aggregation_factory = tff.learning.model_update_aggregator.dp_aggregator(\n",
        "        noise_multiplier,\n",
        "        clients_per_round,\n",
        "        clipped_count_stddev=clipped_count_stddev\n",
        "    )\n",
        "    total_clients = len(train_data.client_ids)\n",
        "    # Poisson subsampling probability\n",
        "    sampling_prob = clients_per_round / total_clients\n",
        "\n",
        "    # Build federated averaging process\n",
        "    learning_process = tff.learning.algorithms.build_unweighted_fed_avg(\n",
        "        my_model_fn,\n",
        "        client_optimizer_fn=tff.learning.optimizers.build_sgdm(0.01),\n",
        "        server_optimizer_fn=tff.learning.optimizers.build_sgdm(1.0, momentum=0.9),\n",
        "        model_aggregator=aggregation_factory\n",
        "    )\n",
        "\n",
        "    eval_process = tff.learning.algorithms.build_fed_eval(my_model_fn)\n",
        "    state = learning_process.initialize()\n",
        "    eval_state = eval_process.initialize()\n",
        "\n",
        "    records = []\n",
        "\n",
        "    for round in range(rounds):\n",
        "        if round % 5 == 0:\n",
        "            model_weights = learning_process.get_model_weights(state)\n",
        "            eval_state = eval_process.set_model_weights(eval_state, model_weights)\n",
        "            eval_output = eval_process.next(eval_state, [test_data])\n",
        "\n",
        "            metrics = eval_output.metrics\n",
        "            print(f'Round {round:3d}: {metrics}')\n",
        "            records.append({'Round': round, 'NoiseMultiplier': noise_multiplier, **metrics})\n",
        "\n",
        "        x = np.random.uniform(size=total_clients)\n",
        "        sampled_clients = [train_data.client_ids[i] for i in range(total_clients) if x[i] < sampling_prob]\n",
        "\n",
        "        if not sampled_clients:\n",
        "            sampled_clients = [train_data.client_ids[np.random.randint(total_clients)]]\n",
        "\n",
        "        sampled_train_data = [train_data.create_tf_dataset_for_client(client) for client in sampled_clients]\n",
        "\n",
        "        result = learning_process.next(state, sampled_train_data)\n",
        "        state = result.state\n",
        "\n",
        "    model_weights = learning_process.get_model_weights(state)\n",
        "    eval_state = eval_process.set_model_weights(eval_state, model_weights)\n",
        "    eval_output = eval_process.next(eval_state, [test_data])\n",
        "    metrics = eval_output.metrics\n",
        "    print(f'Round {rounds:3d}: {metrics}')\n",
        "    records.append({'Round': rounds, 'NoiseMultiplier': noise_multiplier, **metrics})\n",
        "\n",
        "    data_frame = pd.concat([data_frame, pd.DataFrame.from_records(records)], ignore_index=True)\n",
        "    return data_frame\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcaBxl0AbLTQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training with noise multiplier: 0.0\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "dp_aggregator() got an unexpected keyword argument 'clipped_count_stddev'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m noise_multiplier \u001b[38;5;129;01min\u001b[39;00m [\u001b[32m0.0\u001b[39m, \u001b[32m0.5\u001b[39m, \u001b[32m0.75\u001b[39m, \u001b[32m1.0\u001b[39m]:\n\u001b[32m      6\u001b[39m   \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mStarting training with noise multiplier: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnoise_multiplier\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m   data_frame = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise_multiplier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclients_per_round\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_frame\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m   \u001b[38;5;28mprint\u001b[39m()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(rounds, noise_multiplier, clients_per_round, data_frame)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain\u001b[39m(rounds, noise_multiplier, clients_per_round, data_frame):\n\u001b[32m      2\u001b[39m     \u001b[38;5;66;03m# Increase clipped_count_stddev to a larger value for stability\u001b[39;00m\n\u001b[32m      3\u001b[39m     clipped_count_stddev = \u001b[38;5;28mmax\u001b[39m(\u001b[32m1.0\u001b[39m, \u001b[32m0.5\u001b[39m * clients_per_round)  \u001b[38;5;66;03m# increase multiplier here\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     aggregation_factory = \u001b[43mtff\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearning\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_update_aggregator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdp_aggregator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnoise_multiplier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclients_per_round\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclipped_count_stddev\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclipped_count_stddev\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m     \u001b[38;5;66;03m# Poisson subsampling probability\u001b[39;00m\n\u001b[32m     12\u001b[39m     sampling_prob = clients_per_round / total_clients\n",
            "\u001b[31mTypeError\u001b[39m: dp_aggregator() got an unexpected keyword argument 'clipped_count_stddev'"
          ]
        }
      ],
      "source": [
        "data_frame = pd.DataFrame()\n",
        "rounds = 15\n",
        "clients_per_round = 5\n",
        "\n",
        "for noise_multiplier in [0.0, 0.5, 0.75, 1.0]:\n",
        "  print(f'Starting training with noise multiplier: {noise_multiplier}')\n",
        "  data_frame = train(rounds, noise_multiplier, clients_per_round, data_frame)\n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8h5cZ2OUmUF"
      },
      "source": [
        "Now we can visualize the evaluation set accuracy and loss of those runs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHKzgJiQSxAE"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "def make_plot(data_frame):\n",
        "  plt.figure(figsize=(15, 5))\n",
        "\n",
        "  dff = data_frame.rename(\n",
        "      columns={'sparse_categorical_accuracy': 'Accuracy', 'loss': 'Loss'}\n",
        "  )\n",
        "\n",
        "  plt.subplot(121)\n",
        "  sns.lineplot(\n",
        "      data=dff, x='Round', y='Accuracy', hue='NoiseMultiplier', palette='dark'\n",
        "  )\n",
        "  plt.subplot(122)\n",
        "  sns.lineplot(\n",
        "      data=dff, x='Round', y='Loss', hue='NoiseMultiplier', palette='dark'\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xiF8a2XxMt_8"
      },
      "outputs": [],
      "source": [
        "make_plot(data_frame)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jeTMaZ5MunO"
      },
      "source": [
        "It appears that with 50 expected clients per round, this model can tolerate a noise multiplier of up to 0.5 without degrading model quality. A noise multiplier of 0.75 seems to cause a bit of model degradation, and 1.0 makes the model diverge.\n",
        "\n",
        "There is typically a tradeoff between model quality and privacy. The higher noise we use, the more privacy we can get for the same amount of training time and number of clients. Conversely, with less noise, we may have a more accurate model, but we'll have to train with more clients per round to reach our target privacy level.\n",
        "\n",
        "With the experiment above, we might decide that the small amount of model deterioration at 0.75 is acceptable in order to train the final model faster, but let's assume we want to match the performance of the 0.5 noise-multiplier model.\n",
        "\n",
        "Now we can use dp_accounting functions to determine how many expected clients per round we would need to get acceptable privacy. Standard practice is to choose delta somewhat smaller than one over the number of records in the dataset. This dataset has 3383 total training users, so let's aim for (2, 1e-5)-DP.\n",
        "\n",
        "We use `dp_accounting.calibrate_dp_mechanism` to search over the number of clients per round. The privacy accountant (`RdpAccountant`) we use to estimate privacy given a `dp_accounting.DpEvent` is based on [Wang et al. (2018)](https://arxiv.org/abs/1808.00087) and [Mironov et al. (2019)](https://arxiv.org/pdf/1908.10530.pdf)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDMUmKWKXAMB"
      },
      "outputs": [],
      "source": [
        "total_clients = 3383\n",
        "noise_to_clients_ratio = 0.01\n",
        "target_delta = 1e-5\n",
        "target_eps = 2\n",
        "\n",
        "# Initialize arguments to dp_accounting.calibrate_dp_mechanism.\n",
        "\n",
        "# No-arg callable that returns a fresh accountant.\n",
        "make_fresh_accountant = dp_accounting.rdp.RdpAccountant\n",
        "\n",
        "\n",
        "# Create function that takes expected clients per round and returns a\n",
        "# dp_accounting.DpEvent representing the full training process.\n",
        "def make_event_from_param(clients_per_round):\n",
        "  q = clients_per_round / total_clients\n",
        "  noise_multiplier = clients_per_round * noise_to_clients_ratio\n",
        "  gaussian_event = dp_accounting.GaussianDpEvent(noise_multiplier)\n",
        "  sampled_event = dp_accounting.PoissonSampledDpEvent(q, gaussian_event)\n",
        "  composed_event = dp_accounting.SelfComposedDpEvent(sampled_event, rounds)\n",
        "  return composed_event\n",
        "\n",
        "\n",
        "# Create object representing the search range [1, 3383].\n",
        "bracket_interval = dp_accounting.ExplicitBracketInterval(1, total_clients)\n",
        "\n",
        "# Perform search for smallest clients_per_round achieving the target privacy.\n",
        "clients_per_round = dp_accounting.calibrate_dp_mechanism(\n",
        "    make_fresh_accountant,\n",
        "    make_event_from_param,\n",
        "    target_eps,\n",
        "    target_delta,\n",
        "    bracket_interval,\n",
        "    discrete=True,\n",
        ")\n",
        "\n",
        "noise_multiplier = clients_per_round * noise_to_clients_ratio\n",
        "print(\n",
        "    f'To get ({target_eps}, {target_delta})-DP, use {clients_per_round} '\n",
        "    f'clients with noise multiplier {noise_multiplier}.'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExhsP4nxiaok"
      },
      "source": [
        "Now we can train our final private model for release.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIacNzuxibOB"
      },
      "outputs": [],
      "source": [
        "rounds = 100\n",
        "noise_multiplier = 1.2\n",
        "clients_per_round = 120\n",
        "\n",
        "data_frame = pd.DataFrame()\n",
        "data_frame = train(rounds, noise_multiplier, clients_per_round, data_frame)\n",
        "\n",
        "make_plot(data_frame)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-N7cvpVkFR6"
      },
      "source": [
        "As we can see, the final model has similar loss and accuracy to the model trained without noise, but this one satisfies (2, 1e-5)-DP."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "federated_learning_with_differential_privacy.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
